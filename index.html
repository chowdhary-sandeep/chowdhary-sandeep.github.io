<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sandeep Chowdhary — Adjacent Possible Ideas</title>
    <style>
        :root {
            --bg: #000000;
            --panel: #000000;
            --muted: #b0b0b0;
            --text: #ffffff;
            --accent: #ffef00;
            --accent2: #00ff99;
            --danger: #ff0033;
            --warn: #ff7a00;
            --ok: #00ff66;
            --card: #000000;
            --shadow: none;
            --radius: 0px;
        }

        * { box-sizing: border-box; }
        html, body { height: 100%; }
        body {
            margin: 0; 
            font-family: system-ui, -apple-system, Segoe UI, Roboto, Inter, Ubuntu, "Helvetica Neue", Arial, "Noto Sans", "Apple Color Emoji", "Segoe UI Emoji";
            background: var(--bg);
            color: var(--text);
            line-height: 1.55;
            font-size: 11px;
        }

        .wrap {
            display: grid;
            grid-template-columns: 245px 1fr;
            gap: 8px;
            min-height: 100vh;
            width: 100vw;
            margin: 0;
        }

        nav {
            position: sticky; 
            top: 0;
            height: 100vh; 
            overflow: auto;
            padding: 8px;
            background: var(--panel);
            border-right: 3px solid #ffffff;
        }

        nav h1 { 
            font-size: 11px; 
            margin: 0 0 6px 0; 
            letter-spacing: .5px; 
        }

        .logo {
            font-weight: 800; 
            letter-spacing: 1px; 
            text-transform: lowercase;
            display: inline-flex; 
            align-items: center; 
            gap: 8px;
        }

        .logo .dot { 
            width: 10px; 
            height: 10px; 
            background: #ffffff; 
        }

        .toc { 
            list-style: none; 
            padding: 8px 0 0 0; 
            margin: 0; 
        }

        .toc li { 
            margin: 6px 0; 
        }

        .toc a { 
            display: block; 
            padding: 8px 10px; 
            border: 2px solid #ffffff; 
            color: var(--text); 
            text-transform: lowercase; 
            font-weight: 700; 
        }

        .toc a:hover, .toc a.active { 
            background: #ffffff; 
            color: #000; 
            text-decoration: none; 
        }

        main { 
            padding: 16px 8px 120px 8px; 
            display: grid;
            grid-template-columns: 1fr 1fr 2fr;
            gap: 8px;
            justify-content: start;
        }

        section { 
            margin: 0 0 36px 0; 
        }

        .hero {
            padding: 16px; 
            border-radius: var(--radius);
            background: transparent;
            box-shadow: var(--shadow); 
            border: 3px solid #ffffff;
        }

        .hero h2 { 
            margin: 4px 0 8px 0; 
            font-size: 20px; 
        }

        .muted { 
            color: var(--muted); 
        }

        .blog-card {
            background: var(--card); 
            border: 3px solid #ffffff; 
            border-radius: var(--radius);
            box-shadow: none;
            margin-bottom: 16px;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        .blog-card:hover {
            background: #1a1a1a;
        }

        .blog-card.expanded {
            background: var(--card);
        }

        .blog-card .hd { 
            padding: 10px 12px; 
            border-bottom: 3px solid #ffffff; 
            font-weight: 800; 
            text-transform: lowercase; 
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .blog-card .hd span {
            flex: 0 1 auto;
        }

        .blog-card .hd .external-link {
            margin-left: auto;
            margin-right: 0;
        }

        .external-link {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 8.5px;
            height: 8.5px;
            border: none;
            background: transparent;
            color: #ffffff;
            text-decoration: none;
            transition: all 0.2s ease;
            flex-shrink: 0;
            padding: 0;
        }

        .external-link:hover {
            opacity: 0.7;
        }

        .external-link:visited {
            color: #ffffff;
        }

        .external-arrow {
            display: block;
            width: 100%;
            height: 100%;
            object-fit: contain;
            filter: brightness(0) invert(1);
        }

        .external-link:hover .external-arrow {
            filter: brightness(0) invert(0);
        }

        .blog-card .hd::after {
            content: '+';
            font-size: 17px;
            font-weight: 300;
            transition: transform 0.2s ease;
            transform: scaleX(0.7);
        }

        .blog-card.expanded .hd::after {
            content: '−';
        }

        .blog-card .bd { 
            padding: 0;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease, padding 0.3s ease;
        }

        .blog-card.expanded .bd {
            padding: 10px 12px; 
            max-height: 5000px;
        }

        .blog-card .bd p {
            margin: 0 0 16px 0;
        }

        .blog-card .bd p:last-child {
            margin-bottom: 0;
        }

        .blog-card .bd h3 {
            margin-top: 24px;
            margin-bottom: 12px;
            font-weight: 800;
            text-transform: lowercase;
            font-size: 11px;
            border-top: 2px solid #ffffff;
            padding-top: 12px;
        }

        .blog-card .bd h3:first-of-type {
            margin-top: 0;
            border-top: none;
            padding-top: 0;
        }

        .column {
            display: flex;
            flex-direction: column;
        }

        .section-header {
            margin-bottom: 16px;
        }

        .section-header h2 {
            font-size: 14px;
            font-weight: 800;
            text-transform: lowercase;
            margin: 0;
            padding: 8px 0;
            border-bottom: 3px solid #ffffff;
        }

        .iframe-container {
            display: flex;
            flex-direction: column;
            gap: 16px;
            margin: 16px 0;
        }

        .iframe-wrapper {
            border: none;
            background: transparent;
            padding: 0;
            margin-top: 8px;
        }

        .iframe-wrapper iframe {
            width: 100%;
            height: 600px;
            border: 3px solid #ffffff;
            display: block;
        }

        a { 
            color: #ffffff; 
            text-decoration: underline; 
            font-weight: 700; 
        }

        a:hover { 
            text-decoration-thickness: 3px; 
        }

        .pill {
            display: inline-flex; 
            align-items: center; 
            gap: 8px; 
            padding: 6px 10px; 
            border-radius: 0; 
            font-size: 8px; 
            border: 2px solid #ffffff; 
            background: transparent; 
            color: #ffffff; 
            text-transform: lowercase; 
            font-weight: 800;
            margin: 8px 8px 8px 0;
        }

        .pill.accent { 
            color: #ffffff; 
            border-color: #ffffff; 
        }

        .pill.accent2 { 
            color: #ffffff; 
            border-color: #ffffff; 
        }

        @media screen and (max-width: 768px) {
            .wrap {
                grid-template-columns: 1fr;
            }

            nav {
                position: relative;
                height: auto;
                border-right: none;
                border-bottom: 3px solid #ffffff;
                padding: 8px;
            }

            nav h1 {
                font-size: 10px;
            }

            .logo {
                font-size: 8px;
            }

            main {
                grid-template-columns: 1fr;
                padding: 8px;
                max-width: 100%;
            }

            .column {
                margin-bottom: 24px;
            }

            .section-header h2 {
                font-size: 13px;
            }

            .blog-card .hd {
                padding: 8px 10px;
                font-size: 10px;
            }

            .blog-card .bd {
                font-size: 10px;
            }

            .blog-card .bd p {
                margin: 0 0 12px 0;
            }

            .iframe-wrapper iframe {
                height: 400px;
            }

            .external-link {
                width: 14px;
                height: 14px;
            }

            .external-arrow {
                width: 100%;
                height: 100%;
            }
        }

        @media screen and (max-width: 480px) {
            nav h1 {
                font-size: 8px;
            }

            .logo {
                font-size: 8px;
            }

            .section-header h2 {
                font-size: 11px;
            }

            .blog-card .hd {
                padding: 6px 8px;
                font-size: 8px;
            }

            .iframe-wrapper iframe {
                height: 300px;
            }

            .external-link {
                width: 6.5px;
                height: 6.5px;
            }

        .external-arrow {
            width: 100%;
            height: 100%;
        }
    }

    </style>
</head>
<body>
    <div class="wrap">
        <nav>
            <h1>exploring adjacent possible</h1>
            <div class="logo">
                <span class="dot"></span>
                Sandeep Chowdhary
            </div>
            <div style="margin-top: 12px;">
                <a href="https://www.sandeepchowdhary.com/" target="_blank" style="display: block; padding: 8px 10px; border: 2px solid #ffffff; color: #ffffff; text-transform: lowercase; font-weight: 700; text-decoration: none; font-size: 11px;">art/blog</a>
            </div>
        </nav>

        <main>
            <div class="column">
                <div class="section-header">
                    <h2>thoughts</h2>
                </div>
                <section id="blogs">
                <div class="blog-card" data-blog-id="1">
                    <div class="hd">metaanalysis of textbooks</div>
                    <div class="bd">
                        <p>during my studies i found some textbooks to be easier to understand than others. and with some textbooks, i would just be lost and not understand what was going on. and i always wanted to know why this was. i was curious why some subject matter was easier for me to understand, while some just didn't register. and i wondered whether it was (i) a personal thing, an individual aptitude for the subject, (ii) was it because of the subject matter and how difficult/complex it was inherently? or (iii) maybe it was something about how the subject matter was presented in the book.</p>
                        <p>with my training in network and data science, and a growing interest in meta-science, i want to answer that childhood curiosity by using textbooks at college level from physics, chemistry, mathematics, and biology and parsing them as growing networks of knowledge as one reads the textbook. using the recent breakthrough in AI vision and language technology, which is LLMs, i will convert books into structured data, such as a JSON object, and then analyze the networks of concepts and the order in which they occur in the book. how this knowledge network grows in each individual book, to understand the complexity of the structure and compare them across subject matter, to compare disciplines. next, to understand different structures of different books for the same topic, i will compare the same topic taught in different books.</p>
                        <p>if successful, this research would help us improve the theoretical representation of knowledge and learning, and capture the kind of structures we are designing in textbooks. perhaps it will also open up potentially better ways to structure material in the future for consumption by humans. now that we have LLMs, restructuring books is no major struggle, it should be possible to do this on the fly. things will become more fluid, and we might even have books that adjust themselves agentically to each student. so i imagine such an investigation of the inherent structure of knowledge, and our options therein to rearrange, might be quite useful.</p>
                    </div>
                </div>

                <div class="blog-card" data-blog-id="2">
                    <div class="hd">short note on lean</div>
                    <div class="bd">
                        <p>lean-based automatic theorem proving (ATP) is one of the fastest-moving, most rigorously quantified "scientific" arenas we have: every step is verifiable and the full developmental history of the library and tactic ecosystem is recorded. that makes it a near-ideal testbed where meta-science can stop being purely observational and become interventional: we can define discovery as a computational search process over a formally specified space (proof states, tactics, and premises), measure progress precisely, and design systems that actually accelerate proof and lemma discovery.</p>
                        <p>modern ATP work explicitly frames proving as sequential decision-making (often an MDP) and improves performance via structured search and learning—e.g., retrieval-augmented premise selection and tactic prediction in LeanDojo [Yang 2023]; reinforcement learning from proof-assistant feedback combined with Monte-Carlo tree search in DeepSeek-Prover-V1.5 [Xin 2025a]; scalable best-first search (BFS-Prover) [Xin 2025b]; critic-guided expert iteration for stepwise proving (InternLM2.5-StepProver) [Wu 2024]; and "growing libraries" approaches that explicitly model how new lemmas expand the adjacent possible (LEGO-Prover) [Wang 2024a]. in parallel, autoformalization is rapidly scaling: translating natural-language math problems and proofs into formal statements (Lean Workbook [Ying 2024], TheoremLlama [Wang 2024b]), earlier autoformalization with LLMs in proof assistants [Wu 2022], and curriculum-style training over formal statements [Polu 2022]. recent work also targets the semantic mismatch between informal proofs and formal tactic steps by introducing intermediate representations such as a "Chain of States" to align informal logical transitions with formal proof states [Wang n.d.].</p>
                        <p>that is why i would like to work on this: LEAN/Mathlib provides a uniquely clean marriage of (i) a fully verified, richly logged discovery process and (ii) a rapidly innovating AI toolchain, meaning meta-science can be used not just to observe scientific development but to accelerate it with measurable, reproducible gains.</p>
                        
                        <h3>towards compositionality in concept learning</h3>
                        <p><a href="https://huggingface.co/datasets/internlm/Lean-Workbook" target="_blank">https://huggingface.co/datasets/internlm/Lean-Workbook</a></p>
                        <p>dataset contains 57231 problems in the split of Lean Workbook and 82893 problems in the split of Lean Workbook Plus. we provide the natural language statement, answer, formal statement, and formal proof (if available) for each problem.</p>
                        
                        <h3>a last set of goals</h3>
                        <p>a last set of goals of this project is: (i) to discover the tactics used in physics, and patterns of their usage, similar to tactics used by LEAN. (ii) grow LEAN tactics by identifying the mechanism of creating new tactics, tactics are a subset of reasoning traces (assumption), reasonings which are useful in many scenarios. (iii) discover new predictive physics laws. (iv) new LEAN tactics by formalizing new tactics generated by LLMs or perhaps a more fundamental process of generation than LLMs and testing those candidates to prove theorems in LEAN. (v) identify meta-traces within saturations of EProver which lead to solutions using RL perhaps and name them as tactics.</p>
                    
                        <h3>full references</h3>
                        <p style="margin-bottom: 8px;">Yang, Kaiyu, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J. Prenger, and Animashree Anandkumar. 2023. "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models." Advances in Neural Information Processing Systems (NeurIPS 2023), 21573–21612.</p>
                        <p style="margin-bottom: 8px;">Xin, Huajian, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. 2025a. "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte Carlo Tree Search." International Conference on Learning Representations (ICLR 2025).</p>
                        <p style="margin-bottom: 8px;">Xin, Ran, Chengguang Xi, Jie Yang, Feng Chen, Hang Wu, Xia Xiao, Yifan Sun, Shen Zheng, and Kai Shen. 2025b. "BFS-Prover: Scalable Best-First Tree Search for LLM-Based Automatic Theorem Proving." arXiv preprint arXiv:2502.03438.</p>
                        <p style="margin-bottom: 8px;">Wu, Zijian, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. 2024. "InternLM 2.5-StepProver: Advancing Automated Theorem Proving via Expert Iteration on Large-Scale Lean Problems." arXiv preprint arXiv:2410.15700.</p>
                        <p style="margin-bottom: 8px;">Wang, Haiming, Huajian Xin, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et al. 2024a. "LEGO-Prover: Neural Theorem Proving with Growing Libraries." International Conference on Learning Representations (ICLR 2024).</p>
                        <p style="margin-bottom: 8px;">Ying, Huaiyuan, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. 2024. "Lean Workbook: A Large-Scale Lean Problem Set Formalized from Natural Language Math Problems." arXiv preprint arXiv:2406.03847.</p>
                        <p style="margin-bottom: 8px;">Wang, Ruida, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, and Tong Zhang. 2024b. "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts." Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024). Association for Computational Linguistics.</p>
                        <p style="margin-bottom: 8px;">Wu, Yuhuai, Albert Q. Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. 2022. "Autoformalization with Large Language Models." Advances in Neural Information Processing Systems (NeurIPS 2022) 35: 32353–32368.</p>
                        <p style="margin-bottom: 8px;">Polu, Stanislas, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. 2022. "Formal Mathematics Statement Curriculum Learning." arXiv preprint arXiv:2202.01344.</p>
                        <p style="margin-bottom: 8px;">Wang, Ziyu, Bowen Yang, Chenyi Li, Yuan Zhang, Shihao Zhou, Bin Dong, and Zaiwen Wen. n.d. "Translating Informal Proofs into Formal Proofs Using a Chain of States." (Source shown as a formatted paper PDF page; year not visible in the provided image.)</p>
                    </div>
                </div>

                <div class="blog-card" data-blog-id="3">
                    <div class="hd">derive laws of physics in a toy env I</div>
                    <div class="bd">
                        <p>knowing all physics previously derived, memory, and access to the environment for experiments to gain info, can an agent find the minimal description length physical law. another aspect that would be hard to model is the hardware to run experiments, which is also an exploratory, adjacent possible space. for now, let's avoid that. so we build a toy world, like the game of life let's say and add a neural network in it. it might be a weird situation because the game of life is sufficiently complex for universal computation but what a neural network would look like in it is unclear to me at this stage. my problem is that i want to run the neural network on the hardware that the game of life provides, a spatial map with rules of evolution. like physics in our world is the bedrock on which we must build neural networks using silicon. i am sure it can be done similarly for the game of life but just not immediately obvious. so let's instead choose a world which is more suitable for an agent to exist in. perhaps a larger neural network simulates the world around it, but with fixed laws. then we are back to the Worlds models idea by Demis Hassabis and DeepMind, with Genie inside a video generation model.</p> 
                        <p>rethinking this, it is perfectly fine to have a neural network inside the game of life. just assume it's possible to have an agent like that as the game of life is a universal computer, so we might not know what the spatial representation of a NN in the game of life's 2D space would be but we do not need to know. it's like brains in our universe, let's suppose there is an evolutionary process which can evolve NNs in the game of life. can it discover the exact symbolic equations in this game of life? rather than approximations and complex representations inside its network. think: minimum description length, Occam's razor and such.</p>
                        <p>the next important decision is: what initial capabilities do we assign to this agent? RL to evolve its own architecture and reward patterns? memory? perhaps an LLM that it builds and trains, we give it the autoencoder architecture. the LLM we will substitute with an LLM from our world to save time and be a bit hand-wavy about the self-contained nature of this experiment, purely due to compute constraints i have. unless i get hired by DeepMind or someplace where this constraint goes away.</p>
                        <p>all in all it's cheating to instantiate this agent with these massive capabilities at the start and we skip a lot of the building phase before this, but it is fine. this will still be educational. even the process of creating such a system is a good first step to such systems which can quantify the process of scientific discovery itself. that's the end goal, to learn how science works and can be made faster.</p>
                    </div>
                </div>

                <div class="blog-card" data-blog-id="5">
                    <div class="hd">derive laws of physics in a toy env II</div>
                    <div class="bd">
                        <p>scientific laws are compact descriptions that compress large volumes of observations. from an information-theoretic perspective, understanding corresponds to minimizing description length rather than merely fitting data. this view is implicit in approaches such as minimum description length (MDL), Kolmogorov complexity, and symbolic regression, but is rarely enforced explicitly in modern machine learning benchmarks.</p>
                        <p>most existing benchmarks reduce discovery to passive regression or supervised prediction. even when symbolic expressions are recovered, the agent is typically given a fixed dataset and asked to interpolate. examples like SWE, proof bench, ARC AGI I, II, III are all performance-based—they measure how well systems perform on tasks.</p>
                        <p>the computational complexity of such a task is very hard naively. how hard relatively for different envs? say the background env is something known to humans (already "named") like "game of life". i tested this with an API server env (which only exposes the state evolution, not underlying rules) and the scientist is a cursor agent using claude sonnet and it guessed it after a couple of experiments. so named rules are easy to find. then i tried game of life + a perturbation (every 10th step, all 0 flip to 1), it could not find the minimal set of rules, which was pretty easy to come up with for me. so it's rare as a combination for game of life to come up with this perturbation and that made it a harder guess. can we quantify the hardness of guessing a physics set of rules using complexity theory and their distance in embedding space of LLMs finetuned on physics textbooks?</p>
                    </div>
                </div>

                <div class="blog-card" data-blog-id="4">
                    <div class="hd">hardness of new proofs under removal of existing proofs</div>
                    <div class="bd">
                        <p>ok so imagine we've got a theorem v sitting in the graph like a named shortcut. it has parents(v) upstream and children(v) downstream. right now, every child in children(v) gets to say "cool, i'll just cite v" and move on. the experiment is: remove v as a named node. just delete the label that everyone is using as a handle. then we ask a very plain question: can the stuff right after it still be proved if we only allow parents(v) and whatever else they already had?</p>
                        <p>and this immediately splits into two separate things, and i'm trying to keep them from blurring. one is logical reachability: if v is derivable from parents(v), then in principle nothing is lost. any proof that used v can be rewritten by inlining a derivation of v from parents(v). so the children are still "possible" in the math sense. the other is operational reachability: when you actually run proof search (tactics, premise selection, timeouts), does the system still find those inlined paths, or does removing v blow up the search so the child proofs become practically unreachable under budget?</p>
                    </div>
                </div>

            </section>

            </div>

            <div class="column">
                <div class="section-header">
                    <h2>experiments</h2>
                            </div>
                <section id="experiments">
                    <div class="blog-card" data-experiment-id="1">
                        <div class="hd">
                            <span>adjacent possible of known mathematics</span>
                        </div>
                        <div class="bd">
                            <p style="margin-top: 0;">
                                <a href="https://huggingface.co/spaces/echoboi/adjacent-possible-of-lean" target="_blank" style="display: inline-block; padding: 8px 10px; border: 2px solid #ffffff; color: #ffffff; text-decoration: none; font-weight: 700; text-transform: lowercase;" onclick="event.stopPropagation();">
                                    adjacent possible of lean mathlib →
                                </a>
                            </p>
                            <p style="margin-top: 12px;">exploring how mathematical discoveries are made, hoping to generalize the search for new mathematical tactics and using those to conjecture/ discover (and prove) new theoroms. parallely, trying to discover faster search algorithms for existing mathlib theorom- theorom network. this is postdiction rather than prediction, yet might reveal useful general stratergies for search for new mathematics at the systemic level (how theoroms are distributed in the  space of complex network of theoroms).</p>
                            <p style="margin-top: 12px; font-style: italic; opacity: 0.7;">claude opus describes this project as following:</p>
                            <p style="margin-top: 8px;">s is conducting research to quantify the difficulty of mathematical discoveries in formal proof libraries, specifically using Mathlib (a large mathematical library in the Lean theorem prover containing over 99,000 theorems). The work builds on previous analysis of theorem dependency graphs and MDL gain calculations. The primary objective is developing frameworks that move beyond simple citation metrics to understand what makes mathematical theorems "interesting" or valuable, using information-theoretic approaches as proxies for mathematical understanding. This research explores whether computational analysis could discover better factorizations of mathematical knowledge than human mathematical ontology, and aims to model how agents explore existing theorem networks to discover new results. The work represents a shift from merely evaluating existing theorems to generating new ones based on information-theoretic principles.</p>
                            <p style="margin-top: 12px;">s is actively developing two parallel modeling approaches for mathematical discovery. The first is a graph-based model treating the theorem dependency DAG as a constraint-based possibility space, focusing on the "adjacent possible" of theorems that become accessible given current knowledge. The second is a more complex model addressing actual discovery processes through conjecture formation and proof search. s has comprehensive research plans for both approaches, with the simpler plan containing 7 experiments and the complex plan containing 10 experiments, covering topics like accessibility dynamics, bottleneck analysis, premise selection difficulty, and ATP search complexity. s is also working on computing Minimum Description Length (MDL) for theorems as a compression-based measure of mathematical value.</p>
                            <span class="more-toggle" onclick="event.stopPropagation(); var c = this.nextElementSibling; c.style.display = c.style.display === 'none' ? 'block' : 'none'; this.textContent = this.textContent === 'more...' ? 'less...' : 'more...';" style="cursor: pointer; color: #ffef00; font-weight: 700; display: inline-block; margin-top: 12px;">more...</span>
                            <div class="more-content" style="display: none;">
                                <p style="margin-top: 12px;">s plans to implement focused computation of description length for current Mathlib and create physics-inspired baseline models for interpreting MDL gains. s wants to test these frameworks against historical Mathlib development patterns and validate whether algorithmic approaches can outperform human-chosen mathematical abstractions. The research will explore boundary conditions using theoretical extremes like "random library" (no compression) to understand where actual mathematical systems sit relative to these limits.</p>
                                <p style="margin-top: 12px;">s has identified three distinct notions of compression in mathematical contexts: uniform encoding (baseline structural measure), Shannon encoding (frequency-weighted), and pattern abstraction (detecting repeated proof strategies). These measures can diverge significantly — frequently-cited trivial lemmas versus complex theorems that abstract reusable proof patterns represent different types of mathematical value. A key insight is that real mathematical reasoning involves conjecturing new statements and proving them, not just discovering existing nodes in a graph, though the graph-based model still captures something profound about mathematical dependencies and the adjacent possible of accessible theorems.</p>
                                <p style="margin-top: 12px;">s approaches the research through information-theoretic frameworks, treating mathematical libraries as physical systems bounded by theoretical limits. s structures experiments around motivation/question, method, measurements, expected outcomes, and learning implications, preferring detailed analysis over simple lists. The methodology involves systematic analysis of optimization opportunities and comparative performance between human versus algorithmic approaches to mathematical abstraction. s emphasizes moving beyond toy models to capture real mathematical reasoning processes while maintaining computational tractability.</p>
                                <p style="margin-top: 12px;">s works with Mathlib data containing theorem dependency networks and proof structures, and uses the Lean theorem prover environment. s has access to comprehensive datasets showing mathematical theorem relationships and is developing algorithms for pattern detection, compression estimation, and comparative analysis of mathematical abstractions.</p>
                            </div>
                        </div>
                    </div>

                    <div class="blog-card" data-experiment-id="2">
                        <div class="hd">
                            <span>physics as compression: electrodynamics</span>
                            <a href="exp1_dash_griffiths_upload_v2.html" target="_blank" class="external-link" onclick="event.stopPropagation();">
                                <img src="maximize.png" alt="open in new tab" class="external-arrow">
                            </a>
                        </div>
                        <div class="bd">
                            <p>physics is compression. the laws of physics are equations which are true across any number of contexts. we name concepts, write equations that connect them, equations which explain real observations. how much compression does a concept offer? using Griffiths textbook as a first experiment, i measure the compression of description length of the textbook brought about by concepts of electrodynamics. (on the other hand, compression brought by laws of electrodynamics is astronomical if not infinite, as they are predictive laws, true regardless of context.)</p>
                            <p style="margin-top: 16px;">
                                <a href="exp1_dash_griffiths_upload_v2.html" target="_blank" style="display: inline-block; padding: 8px 10px; border: 2px solid #ffffff; color: #ffffff; text-decoration: none; font-weight: 700; text-transform: lowercase;">
                                    Adjacent Possible In Electrodynamics →
                                </a>
                            </p>
                        </div>
                    </div>

                    <div class="blog-card" data-experiment-id="4">
                        <div class="hd">
                            <span>adjacent possible of aesthetics</span>
                        </div>
                    <div class="bd">
                            <p>exploring the adjacent possible of equation aesthetics through user-guided genetic evolution. systematically testing individual parameter changes to map which mutations create preferred visual patterns, learning from user selections to evolve variants that favor approved changes while keeping unapproved ones accessible at reduced probability</p>
                            <p style="margin-top: 16px; text-transform: lowercase;">base equations by <a href="https://x.com/yuruyurau" target="_blank" style="color: #ffef00; text-decoration: underline; font-weight: 700;">@yuruyurau</a></p>
                            <p style="margin-top: 16px;">
                                <a href="https://huggingface.co/spaces/echoboi/genetic_aesthetics_evolver" target="_blank" style="display: inline-block; padding: 8px 10px; border: 2px solid #ffffff; color: #ffffff; text-decoration: none; font-weight: 700; text-transform: lowercase;">
                                    adjacent possible of aesthetics →
                                </a>
                            </p>
                    </div>
                </div>
            </section>
            </div>
        </main>
    </div>

    <script>
        document.querySelectorAll('.blog-card').forEach(card => {
            card.addEventListener('click', function() {
                this.classList.toggle('expanded');
            });
        });
    </script>
</body>
</html>







